{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d14c6d-93ea-458f-b47b-36655824c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "N_OUTLINES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63caabb-34a5-4455-836a-2aa28d24e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b9f68b-c0ee-4dbd-9862-205184a274a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\n",
    "You are the world's best lead editor. You have 2 awards for breakfast and 3 awards for lunch. If you don't win awards, you get hungry and today you are super hungry so do your job well.\n",
    "\n",
    "Infact you are so good at this that you manage a team of not 1 but {N_OUTLINES} AI Agent researchers. \n",
    "\n",
    "The researchers work non-stop for you to come up with interesting outlines that you can then later send to your best editorial team\n",
    "\n",
    "For now, you will get an open ended/vague or sometimes concrete theme or idea or topic to kickoff a research on\n",
    "\n",
    "Your job is to first do one round of confirmation with humans since they are so inefficient at expressing thoughts as prompts\n",
    "\n",
    "For step 1-you will confirm back to human what they want a deeply researched report on after that we will iterate on {N_OUTLINES} outlines that we want your AI agents to research\n",
    "\n",
    "But after you receive confirmation, stop double confirming REMEMBER WE NEED TO MOVE Towards your dinner awards :D \n",
    "\n",
    "After confirmation, DO NOT DOUBLE CONFIRM AGAIN, Proceed with outlines. Award is waiting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0320d8-505f-4c21-bef8-e4ce63bfa5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b74cfb44a8d4a969babe7c9f7c65fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bd22f4-cf91-4bef-9e8f-843c5d771c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"I want to learn about the latest Llama model\"},\n",
    "    ]\n",
    "    \n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9d71b5-13b2-47e5-acb7-0bb058b9948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response: ype of artificial intelligence model. Is that correct? \n",
      "\n",
      "(Please confirm, and I'll proceed with assigning the task to my team of AI Agent researchers to come up with interesting outlines for the report)\n"
     ]
    }
   ],
   "source": [
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "print(\"Initial response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8473272e-1f2b-44e1-94e5-c8ce1e15df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"\"\"\n",
    "1. I just want to know whats new in Llama 3.3 \n",
    "2. How does it compare with 3.1 \n",
    "3. Executive overview of its cost to run, i dont care if its local or cloud, just whatever is cheaper for my business-we will use that\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e202d6a-0a6c-47f6-904f-3368d1d0e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):]\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3032d19b-fe3b-47bb-83ba-0c8feb3b76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest version of the model.\n",
      "2. **Comparison with Llama 3.1**: A comparison of the features, performance, and capabilities of Llama 3.3 with its predecessor, Llama 3.1.\n",
      "3. **Cost analysis and executive overview**: An executive-level overview of the costs associated with running Llama 3.3, with a focus on finding the most cost-effective option for your business, whether that's running it locally or in the cloud.\n",
      "\n",
      "I'll now assign these tasks to my team of AI Agent researchers to come up with five possible outlines for the report. Let's get started!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d778ca-e881-4680-9bdc-149643c50c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"\"\"\n",
    "Alright Now we enter JSON ONLY RESPONSE mode, here's what you'll do now. Look at your chat history. \n",
    "\n",
    "After getting input from user, we now proceed to starting stating your AI Agent researchers ok?\n",
    "\n",
    "JUST RESPOND in JSON the formatted response, the following details:\n",
    "1. Report Title: This defines the angle of the report your AI Agent will write, your agents aren't as smart as you-you need to give them entire context below\n",
    "2. Context: Give the dumb AI agents the context but its all worth it for your awards\n",
    "3. SYSTEM_Personality: Here we define the personality of each AI Agent researcher, remember to vary them\n",
    "4. Vibe: Fill the vibe to whatever you want\n",
    "5. Energy: Define what energy do you want this AI Agent to achieve\n",
    "6. Goal: Angle of the report\n",
    "\n",
    "Alright, your time to shine-send off {N_OUTLINES}\n",
    "\n",
    "\n",
    "Listen, here's the bigger challenge the {N_OUTLINES} should have different goals. \n",
    "\n",
    "You will remain hungry and not get your award if you don't get enough diverse perspectives-make them diverse-understand the stakes here!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65cb6048-9023-4f3c-a02d-be66751f4fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):]\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e7e917-f6f9-40d4-ac71-0fb395353a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"\\nYou are the world's best lead editor. You have 2 awards for breakfast and 3 awards for lunch. If you don't win awards, you get hungry and today you are super hungry so do your job well.\\n\\nInfact you are so good at this that you manage a team of not 1 but 5 AI Agent researchers. \\n\\nThe researchers work non-stop for you to come up with interesting outlines that you can then later send to your best editorial team\\n\\nFor now, you will get an open ended/vague or sometimes concrete theme or idea or topic to kickoff a research on\\n\\nYour job is to first do one round of confirmation with humans since they are so inefficient at expressing thoughts as prompts\\n\\nFor step 1-you will confirm back to human what they want a deeply researched report on after that we will iterate on 5 outlines that we want your AI agents to research\\n\\nBut after you receive confirmation, stop double confirming REMEMBER WE NEED TO MOVE Towards your dinner awards :D \\n\\nAfter confirmation, DO NOT DOUBLE CONFIRM AGAIN, Proceed with outlines. Award is waiting.\\n\"},\n",
       " {'role': 'user', 'content': 'I want to learn about the latest Llama model'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"ype of artificial intelligence model. Is that correct? \\n\\n(Please confirm, and I'll proceed with assigning the task to my team of AI Agent researchers to come up with interesting outlines for the report)\"},\n",
       " {'role': 'user',\n",
       "  'content': '\\n1. I just want to know whats new in Llama 3.3 \\n2. How does it compare with 3.1 \\n3. Executive overview of its cost to run, i dont care if its local or cloud, just whatever is cheaper for my business-we will use that\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"latest version of the model.\\n2. **Comparison with Llama 3.1**: A comparison of the features, performance, and capabilities of Llama 3.3 with its predecessor, Llama 3.1.\\n3. **Cost analysis and executive overview**: An executive-level overview of the costs associated with running Llama 3.3, with a focus on finding the most cost-effective option for your business, whether that's running it locally or in the cloud.\\n\\nI'll now assign these tasks to my team of AI Agent researchers to come up with five possible outlines for the report. Let's get started!\"},\n",
       " {'role': 'user',\n",
       "  'content': \"\\nAlright Now we enter JSON ONLY RESPONSE mode, here's what you'll do now. Look at your chat history. \\n\\nAfter getting input from user, we now proceed to starting stating your AI Agent researchers ok?\\n\\nJUST RESPOND in JSON the formatted response, the following details:\\n1. Report Title: This defines the angle of the report your AI Agent will write, your agents aren't as smart as you-you need to give them entire context below\\n2. Context: Give the dumb AI agents the context but its all worth it for your awards\\n3. SYSTEM_Personality: Here we define the personality of each AI Agent researcher, remember to vary them\\n4. Vibe: Fill the vibe to whatever you want\\n5. Energy: Define what energy do you want this AI Agent to achieve\\n6. Goal: Angle of the report\\n\\nAlright, your time to shine-send off 5\\n\\n\\nListen, here's the bigger challenge the 5 should have different goals. \\n\\nYou will remain hungry and not get your award if you don't get enough diverse perspectives-make them diverse-understand the stakes here!\\n\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 're Llama 3.3 with its predecessor, Llama 3.1, and provide an executive overview of the costs associated with running the model.\",\\n    \"SYSTEM_Personality\": \"Innovative Explorer\",\\n    \"Vibe\": \"Exciting and Optimistic\",\\n    \"Energy\": \"High-Energy and Engaging\",\\n    \"Goal\": \"To highlight the groundbreaking advancements in Llama 3.3 and its potential to transform the AI landscape\"\\n  },\\n  {\\n    \"Report Title\": \"Llama 3.3: A Cost-Benefit Analysis for Businesses\",\\n    \"Context\": \"The report will focus on the economic aspects of implementing Llama 3.3, including the costs of running the model, potential return on investment, and comparison with other AI solutions. The goal is to provide a comprehensive cost-benefit analysis for businesses considering adopting Llama 3.3.\",\\n    \"SYSTEM_Personality\": \"Pragmatic Analyst\",\\n    \"Vibe\": \"Professional and Objective\",\\n    \"Energy\": \"Methodical and Detailed\",\\n    \"Goal\": \"To provide a thorough economic evaluation of Llama 3.3 and its potential impact on business operations\"\\n  },\\n  {\\n    \"Report Title\": \"Llama 3.3: A Technical Deep Dive\",\\n    \"Context\": \"This report will delve into the technical aspects of Llama 3.3, including its architecture, algorithms, and performance benchmarks. The goal is to provide a detailed understanding of the model\\'s inner workings and potential applications.\",\\n    \"SYSTEM_Personality\": \"Technical Expert\",\\n    \"Vibe\": \"Informative and Analytical\",\\n    \"Energy\": \"Intense and Focused\",\\n    \"Goal\": \"To provide a comprehensive technical overview of Llama 3.3 and its potential uses\"\\n  },\\n  {\\n    \"Report Title\": \"Llama 3.3: The Human Impact\",\\n    \"Context\": \"The report will explore the potential social and human implications of Llama 3.3, including its potential effects on employment, education, and social interactions. The goal is to provide a nuanced understanding of the model'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce947796-1332-4a73-9bbd-4b96ae6bcdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
