{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Calling 101:\n",
    "\n",
    "Note: If you are looking for `3.2` Featherlight Model (1B and 3B) instructions, please see the respective notebook, this one covers 3.1 models.\n",
    "\n",
    "Note: The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. According to https://console.groq.com/docs/deprecations it is replaced with `llama-3.3-70b-versatile`. \n",
    "While these new model deliver improved quality, it may produce different responses than their predecessors.\n",
    "\n",
    "We are briefly introduction the `3.2` models at the end. \n",
    "\n",
    "Note: The new vision models behave same as `3.1` models when you are talking to the models without an image\n",
    "\n",
    "This is part (1/2) in the tool calling series, this notebook will cover the basics of what tool calling is and how to perform it with `Llama 3.3 models`\n",
    "\n",
    "Here's what you will learn in this notebook:\n",
    "\n",
    "- Setup Groq to access Llama 3.3 70B model\n",
    "- Avoid common mistakes when performing tool-calling with Llama\n",
    "- Understand Prompt templates for Tool Calling\n",
    "- Understand how the tool calls are handled under the hood\n",
    "- 3.2 Model Tool Calling Format and Behaviour\n",
    "\n",
    "In Part 2, we will learn how to build system that can get us comparison between 2 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Tool Calling?\n",
    "\n",
    "This approach was popularised by the [Gorilla](https://gorilla.cs.berkeley.edu) paper-which showed that Large Language Model(s) can be fine-tuned on API examples to teach them calling an external API. \n",
    "\n",
    "This is really cool because we can now use a LLM as a \"brain\" of a system and connect it to external systems to perform actions. \n",
    "\n",
    "In simpler words, \"Llama can order your pizza for you\" :) \n",
    "\n",
    "With the Llama 3.1 release, the models excel at tool calling and support out of box `brave_search`, `wolfram_api` and `code_interpreter`. Llama 3.3 supports same functionality.\n",
    "\n",
    "However, first let's take a look at a common mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install and setup groq dependencies\n",
    "\n",
    "- Install `groq` api to access Llama model(s)\n",
    "- Configure our client and authenticate with API Key(s), Note: PLEASE UPDATE YOUR KEY BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install groq\n",
    "%set_env GROQ_API_KEY=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "# Create the Groq client\n",
    "client = Groq(api_key='YOUR_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistake of Tool-Calling: Incorrect Prompt Template\n",
    "\n",
    "While Llama 3.1 works with tool-calling out of box, a wrong prompt template can cause issues with unexpected behaviour. \n",
    "\n",
    "Sometimes, even superheroes need to be reminded of their powers. \n",
    "\n",
    "Let's first try \"forcing a prompt response from the model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Remember this is the WRONG template, please scroll to next section to see the right approach if you are in a rushed copy-pasta sprint\n",
    "\n",
    "This section will show you that the model will not use `brave_search` and `wolfram_api` out of the box unless the prompt template is set correctly. \n",
    "Even if the model is asked to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 13 February 2025\n",
    "\n",
    "You are a helpful assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {}\n",
    "chat_history = []\n",
    "\n",
    "def model_chat(user_input: str, sys_prompt = SYSTEM_PROMPT, temperature: int = 0.7, max_tokens=4096):\n",
    "    \n",
    "    chat_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sys_prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = client.chat.completions.create(model=\"llama-3.3-70b-versatile\",\n",
    "                                          messages=chat_history,\n",
    "                                          max_tokens=max_tokens,\n",
    "                                          temperature=temperature)\n",
    "    \n",
    "    chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response.choices[0].message.content\n",
    "    })\n",
    "    \n",
    "    \n",
    "    #print(\"Assistant:\", response.choices[0].message.content)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking the model about a recent news\n",
    "\n",
    "Since the prompt template is incorrect, it will answer using cutoff memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: As of my knowledge cutoff in December 2023, there hasn't been an official announcement from FromSoftware or Bandai Namco regarding a sequel to Elden Ring. However, in an interview, Hidetaka Miyazaki, the president of FromSoftware, mentioned that the company is already working on a new project, but no details were provided.\n",
      "\n",
      "That being said, there have been rumors and speculations about a potential Elden Ring sequel or DLC (downloadable content) expansion. Some people believe that a new game or expansion might be announced in 2024 or later, but nothing has been confirmed by the developers.\n",
      "\n",
      "It's worth noting that FromSoftware typically takes several years to develop a new game, and they often work on multiple projects simultaneously. Given the massive success of Elden Ring, it's likely that the company will continue to support the game with updates and potentially new content, but a full-fledged sequel might take some time to develop.\n",
      "\n",
      "I recommend keeping an eye on official announcements from FromSoftware and Bandai Namco, as well as following reputable gaming news sources, to stay up-to-date on any future developments regarding Elden Ring or other FromSoftware games.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "When is the next elden ring game coming out?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking the model about a Math problem\n",
    "\n",
    "Again, the model answer(s) based on memory and not tool-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To find the square root of 23131231, I'll calculate it for you.\n",
      "\n",
      "√23131231 ≈ 4807.03\n",
      "\n",
      "So, the square root of 23131231 is approximately 4807.03.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "When is the square root of 23131231?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we solve this using a reminder prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To find the square root of 23131231, I can use a calculator or a computational tool.\n",
      "\n",
      "The square root of 23131231 is: √23131231 = 4817\n",
      "\n",
      "So, the square root of 23131231 is 4817.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "When is the square root of 23131231?\n",
    "\n",
    "Can you use a tool to solve the question?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we didn't get the wolfram_api call, let's try one more time with a stronger prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To find the square root of 23131231, I can use a calculator or a computational tool like Wolfram Alpha.\n",
      "\n",
      "Using Wolfram Alpha, I get:\n",
      "\n",
      "√23131231 ≈ 4817.42\n",
      "\n",
      "So, the square root of 23131231 is approximately 4817.42.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "When is the square root of 23131231?\n",
    "\n",
    "Can you use a tool to solve the question?\n",
    "\n",
    "Remember you have been trained on wolfram_alpha\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official Prompt Template \n",
    "\n",
    "As you can see, the model doesn't perform tool-calling in an expected fashion above. This is because we are not following the recommended prompting format.\n",
    "\n",
    "The (Llama Stack)[https://github.com/meta-llama/llama-stack] is the go to approach to use the Llama model family and build applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To learn about the various prompt formats available \n",
    "\n",
    "See the [official Llama documentation](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/) that includes Model Card and Prompt Template.\n",
    "\n",
    "Let's take a look at Input Prompt Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!llama model prompt-format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Questions: Can you retrieve the details for the user with the ID 7890, who has black as their special request?\n",
    "Here is a list of functions in JSON format that you can invoke:\n",
    "[\n",
    "    {\n",
    "        \"name\": \"get_user_info\",\n",
    "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"dict\",\n",
    "            \"required\": [\n",
    "                \"user_id\"\n",
    "            ],\n",
    "            \"properties\": {\n",
    "                \"user_id\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
    "            },\n",
    "            \"special\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
    "                \"default\": \"none\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "Should you decide to return the function calls, put them in the format [func1(params_name=params_value, params_name2=params_value2...), func2(params)]\n",
    "\n",
    "NO other text MUST be included.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling: Using the correct Prompt Template\n",
    "\n",
    "With the [official Llama documentation](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/) we have already learned the right behaviour of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP: Convert the prompt to return the response from output, adjust functions\n",
    "\n",
    "If everything is setup correctly-the model should now wrap function calls  with the `|<python_tag>|` following the actually function call. \n",
    "\n",
    "This can allow you to manage your function calling logic accordingly. \n",
    "\n",
    "Time to test the theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <|python_tag|>brave_search.call(query=\"Elden Ring sequel release date\")\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Environment: iPython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 15 September 2024\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"\"\"\n",
    "When is the next Elden ring game coming out?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ## Step 1: Analyze the problem\n",
      "The problem is asking for the square root of a given number, 23131231. We need to find the value that, when multiplied by itself, equals this number.\n",
      "\n",
      "## Step 2: Outline the solution strategy\n",
      "To solve this problem, we can use the mathematical concept of square roots and the Python programming language to calculate it. Specifically, we will use the `math.sqrt()` function in Python, which returns the square root of a given number.\n",
      "\n",
      "## Step 3: Calculate the square root of the number\n",
      "We will use the `math.sqrt()` function to calculate the square root of 23131231.\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "# Define the number\n",
      "number = 23131231\n",
      "\n",
      "# Calculate the square root of the number\n",
      "square_root = math.sqrt(number)\n",
      "```\n",
      "\n",
      "## Step 4: Combine the code into a function\n",
      "Once we have finished all the steps, we will combine the Python code from all the steps into a single function. The function should not take any arguments.\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def calculate_square_root():\n",
      "# Define the number\n",
      "number = 23131231\n",
      "\n",
      "# Calculate the square root of the number\n",
      "square_root = math.sqrt(number)\n",
      "\n",
      "return square_root\n",
      "\n",
      "# Execute the function\n",
      "result = calculate_square_root()\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Let's execute the function using the iPython tool.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "What is the square root of 23131231?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this knowledge in practise\n",
    "\n",
    "A common misconception about tool calling is: the model can handle the tool call and get your output. \n",
    "\n",
    "This is NOT TRUE, the actual tool call is something that you have to implement. With this knowledge, let's see how we can utilise brave search to answer our original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install brave-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ## Step 1: Analyze the problem\n",
      "\n",
      "The problem asks us to find the square root of a given number, 23131231.\n",
      "\n",
      "## Step 2: Outline the solution strategy\n",
      "\n",
      "To solve this problem, we can use the built-in math library in Python, which has a function to calculate the square root of a number.\n",
      "\n",
      "## Step 3: Calculate the square root\n",
      "\n",
      "We will use the `math.sqrt()` function from the math library to calculate the square root of 23131231.\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "# Define the number\n",
      "number = 23131231\n",
      "\n",
      "# Calculate the square root\n",
      "square_root = math.sqrt(number)\n",
      "```\n",
      "\n",
      "## Step 4: Print the result\n",
      "\n",
      "Finally, we will print the calculated square root.\n",
      "\n",
      "```python\n",
      "print(square_root)\n",
      "```\n",
      "\n",
      "### Combine the code into a function and execute it using iPython\n",
      "\n",
      "\n",
      "```python\n",
      "```import math\n",
      "\n",
      "def calculate_square_root():\n",
      "# Define the number\n",
      "number = 23131231\n",
      "\n",
      "# Calculate the square root\n",
      "square_root = math.sqrt(number)\n",
      "\n",
      "return square_root\n",
      "\n",
      "# Execute the function\n",
      "result = calculate_square_root()\n",
      "print(result)\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Environment: iPython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 15 September 2024\n",
    "\"\"\"\n",
    "\n",
    "user_input = \"\"\"\n",
    "What is the square root of 23131231?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Step 1: Analyze the Problem\n",
      "The problem asks for the square root of a given number, 23131231.\n",
      "\n",
      "## Step 2: Outline the Solution Strategy\n",
      "To find the square root of 23131231, we can use the mathematical function for square root. In Python, this is available as `math.sqrt()` from the math module.\n",
      "\n",
      "## Step 3: Calculate the Square Root\n",
      "We'll import the necessary module and define the variable before calculating the square root.\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "# Define the number\n",
      "number = 23131231\n",
      "\n",
      "# Calculate the square root of the number\n",
      "square_root = math.sqrt(number)\n",
      "```\n",
      "\n",
      "## Step 4: Combine the Code into a Function\n",
      "Once we have finished all the steps, we combine the Python code into a single function that does not take any arguments.\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "def calculate_square_root():\n",
      "# Define the number\n",
      "number = 23131231\n",
      "\n",
      "# Calculate the square root of the number\n",
      "square_root = math.sqrt(number)\n",
      "\n",
      "return square_root\n",
      "\n",
      "# Execute the function\n",
      "result = calculate_square_root()\n",
      "print(result)\n",
      "```\n",
      "\n",
      "Let's execute this function using the iPython tool:\n"
     ]
    }
   ],
   "source": [
    "print(model_chat(user_input, sys_prompt=SYSTEM_PROMPT))\n",
    "\n",
    "output = model_chat(user_input, sys_prompt=SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Extract the function name\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m fn_name \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m|python_tag\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m|>(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw+)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract the method\u001b[39;00m\n\u001b[1;32m      7\u001b[0m fn_call_method \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m, output)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Extract the function name\n",
    "fn_name = re.search(r'<\\|python_tag\\|>(\\w+)\\.', output).group(1)\n",
    "\n",
    "# Extract the method\n",
    "fn_call_method = re.search(r'\\.(\\w+)\\(', output).group(1)\n",
    "\n",
    "# Extract the arguments\n",
    "fn_call_args = re.search(r'=\\s*([^)]+)', output).group(1)\n",
    "\n",
    "print(f\"Function name: {fn_name}\")\n",
    "print(f\"Method: {fn_call_method}\")\n",
    "print(f\"Args: {fn_call_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement this in different ways but the idea is the same, the LLM gives an output with the `<|python_tag|>`, which should call a tool-calling mechanism. \n",
    "\n",
    "This logic gets handled in the program and then the output is passed back to the model to answer the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code interpreter\n",
    "\n",
    "With the correct prompt template, Llama model can output Python (as well as code in any-language that the model has been trained on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To calculate how long it would take to reach a goal of $100,000 in investments, we can use a formula or create a simple simulation. \n",
      "\n",
      "Assuming the interest rate is 5% per year, compounded monthly, and you invest $400 every month, here's a breakdown:\n",
      "\n",
      "1. Monthly interest rate: 5%/year / 12 months/year = 0.004167 (or approximately 0.417% per month)\n",
      "2. Total amount invested per month: $400\n",
      "3. Initial investment: $0 (assuming you start from scratch)\n",
      "\n",
      "Using a compound interest calculator or creating a simple spreadsheet, we can simulate the growth of your investments over time.\n",
      "\n",
      "Here's a rough estimate:\n",
      "\n",
      "* After 1 year: approximately $4,800 in investments (12 months \\* $400/month) + interest\n",
      "* After 5 years: approximately $24,400 in investments + interest\n",
      "* After 10 years: approximately $53,900 in investments + interest\n",
      "* After 15 years: approximately $83,300 in investments + interest\n",
      "* After 16 years: approximately $93,400 in investments + interest\n",
      "* After 17 years: approximately $103,600 in investments + interest\n",
      "\n",
      "Based on this simulation, it would take approximately 17 years to reach a total of $100,000 in investments, assuming a 5% annual interest rate, compounded monthly, and a monthly investment of $400.\n",
      "\n",
      "Keep in mind that this is a rough estimate and actual results may vary depending on the specific investment vehicle and any fees associated with it. It's always a good idea to consult with a financial advisor or conduct your own research before making investment decisions.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "\n",
    "If I can invest 400$ every month at 5% interest rate, how long would it take me to make a 100k$ in investments?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the output by running the output from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 172 months, approximately 14.33 years, to reach the target amount of $100000.00.\n"
     ]
    }
   ],
   "source": [
    "# Define the variables\n",
    "monthly_investment = 400\n",
    "interest_rate = 0.05\n",
    "target_amount = 100000\n",
    "\n",
    "# Calculate the number of months it would take to reach the target amount\n",
    "months = 0\n",
    "current_amount = 0\n",
    "while current_amount < target_amount:\n",
    "    current_amount += monthly_investment\n",
    "    current_amount *= 1 + interest_rate / 12  # Compound interest\n",
    "    months += 1\n",
    "\n",
    "# Print the result\n",
    "print(f\"It would take {months} months, approximately {months / 12:.2f} years, to reach the target amount of ${target_amount:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Models Custom Tool Prompt Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Life is great because Llama Team writes great docs for us, so we can conveniently copy-pasta examples from there :)\n",
    "\n",
    "[Here](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-tool-calling-(1b/3b)-) are the docs for your reference that we will be using. \n",
    "\n",
    "Exercise for viewer: Use `llama-toolchain` again to verify like we did earlier and then start the prompt engineering for the small Llamas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_definitions = \"\"\"[\n",
    "    {\n",
    "        \"name\": \"get_user_info\",\n",
    "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"dict\",\n",
    "            \"required\": [\n",
    "                \"user_id\"\n",
    "            ],\n",
    "            \"properties\": {\n",
    "                \"user_id\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
    "            },\n",
    "            \"special\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
    "                \"default\": \"none\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert in composing functions. You are given a question and a set of possible functions. \n",
    "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
    "If none of the function can be used, point it out. If the given question lacks the parameters required by the function,\n",
    "also point it out. You should only return the function call in tools call sections.\n",
    "\n",
    "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\n",
    "You SHOULD NOT include any other text in the response.\n",
    "\n",
    "Here is a list of functions in JSON format that you can invoke.\\n\\n{functions}\\n\"\"\".format(functions=function_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def model_chat(user_input: str, sys_prompt = system_prompt, temperature: int = 0.7, max_tokens=2048):\n",
    "    \n",
    "    chat_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = client.chat.completions.create(model=\"llama-3.2-3b-preview\",\n",
    "                                          messages=chat_history,\n",
    "                                          max_tokens=max_tokens,\n",
    "                                          temperature=temperature)\n",
    "    \n",
    "    chat_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response.choices[0].message.content\n",
    "    })\n",
    "    \n",
    "    \n",
    "    #print(\"Assistant:\", response.choices[0].message.content)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We are assuming a structure for dataset here:\n",
    "\n",
    "- Name\n",
    "- Email\n",
    "- Age \n",
    "- Color request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: [get_user_info(user_id=7890, special='black')]\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Can you retrieve the details for the user with the ID 7890, who has black as their special request?\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=system_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy dataset to make sure our model stays happy :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_info(user_id: int, special: str = \"none\") -> dict:\n",
    "    # This is a mock database of users\n",
    "    user_database = {\n",
    "        7890: {\"name\": \"Emma Davis\", \"email\": \"emma@example.com\", \"age\": 31},\n",
    "        1234: {\"name\": \"Liam Wilson\", \"email\": \"liam@example.com\", \"age\": 28},\n",
    "        2345: {\"name\": \"Olivia Chen\", \"email\": \"olivia@example.com\", \"age\": 35},\n",
    "        3456: {\"name\": \"Noah Taylor\", \"email\": \"noah@example.com\", \"age\": 42},\n",
    "        4567: {\"name\": \"Ava Martinez\", \"email\": \"ava@example.com\", \"age\": 39},\n",
    "        5678: {\"name\": \"Ethan Brown\", \"email\": \"ethan@example.com\", \"age\": 45},\n",
    "        6789: {\"name\": \"Sophia Kim\", \"email\": \"sophia@example.com\", \"age\": 33},\n",
    "        8901: {\"name\": \"Mason Lee\", \"email\": \"mason@example.com\", \"age\": 29},\n",
    "        9012: {\"name\": \"Isabella Garcia\", \"email\": \"isabella@example.com\", \"age\": 37},\n",
    "        1357: {\"name\": \"James Johnson\", \"email\": \"james@example.com\", \"age\": 41}\n",
    "    }\n",
    "    \n",
    "    # Check if the user exists in our mock database\n",
    "    if user_id in user_database:\n",
    "        user_data = user_database[user_id]\n",
    "        \n",
    "        # Handle the 'special' parameter\n",
    "        if special != \"none\":\n",
    "            user_data[\"special_info\"] = f\"Special request: {special}\"\n",
    "        \n",
    "        return user_data\n",
    "    else:\n",
    "        return {\"error\": \"User not found\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Emma Davis',\n",
       "  'email': 'emma@example.com',\n",
       "  'age': 31,\n",
       "  'special_info': 'Special request: black'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[get_user_info(user_id=7890, special='black')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Tool-Calling logic for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Regex, my good old friend :) \n",
    "\n",
    "With Regex, we can write a simple way to handle tool_calling and return either the model or tool call response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Assuming you have defined get_user_info function and SYSTEM_PROMPT\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def process_response(response):\n",
    "    function_call_pattern = r'\\[(.*?)\\((.*?)\\)\\]'\n",
    "    function_calls = re.findall(function_call_pattern, response)\n",
    "    \n",
    "    if function_calls:\n",
    "        processed_response = []\n",
    "        for func_name, args_str in function_calls:\n",
    "            args_dict = {}\n",
    "            for arg in args_str.split(','):\n",
    "                key, value = arg.split('=')\n",
    "                key = key.strip()\n",
    "                value = value.strip().strip(\"'\")\n",
    "                if value.isdigit():\n",
    "                    value = int(value)\n",
    "                args_dict[key] = value\n",
    "            \n",
    "            if func_name == 'get_user_info':\n",
    "                result = get_user_info(**args_dict)\n",
    "                processed_response.append(f\"Function call result: {json.dumps(result, indent=2)}\")\n",
    "            else:\n",
    "                processed_response.append(f\"Unknown function: {func_name}\")\n",
    "        return \"\\n\".join(processed_response)\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "def model_chat(user_input: str, sys_prompt=system_prompt, temperature: float = 0.7, max_tokens: int = 2048):\n",
    "    global chat_history\n",
    "    \n",
    "    if not chat_history:\n",
    "        chat_history = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_prompt\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.2-3b-preview\",\n",
    "        messages=chat_history,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    assistant_response = response.choices[0].message.content\n",
    "    processed_response = process_response(assistant_response)\n",
    "    \n",
    "    chat_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_response\n",
    "    })\n",
    "    \n",
    "    return processed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Function call result: {\n",
      "  \"name\": \"Emma Davis\",\n",
      "  \"email\": \"emma@example.com\",\n",
      "  \"age\": 31,\n",
      "  \"special_info\": \"Special request: black\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Can you retrieve the details for the user with the ID 7890, who has black as their special request?\"\n",
    "\n",
    "print(\"Assistant:\", model_chat(user_input, sys_prompt=system_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
