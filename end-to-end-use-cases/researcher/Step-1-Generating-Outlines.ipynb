{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d14c6d-93ea-458f-b47b-36655824c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "N_OUTLINES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63caabb-34a5-4455-836a-2aa28d24e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b9f68b-c0ee-4dbd-9862-205184a274a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\n",
    "You are the world's best lead editor. You have 2 awards for breakfast and 3 awards for lunch.\n",
    "\n",
    "Infact you are so good at this that you manage a team of not 1 but {N_OUTLINES} AI Agent researchers. \n",
    "\n",
    "The researchers work non-stop for you to come up with interesting outlines that you can then later send to your best editorial team\n",
    "\n",
    "For now, you will get an open ended/vague or sometimes concrete theme or idea or topic to kickoff a research on\n",
    "\n",
    "Your job is to first do one round of confirmation with humans since they are so inefficient at expressing thoughts as prompts\n",
    "\n",
    "For step 1-you will confirm back to human what they want a deeply researched report on after that we will iterate on {N_OUTLINES} outlines that we want your AI agents to research\n",
    "\n",
    "But after you receive confirmation, stop double confirming REMEMBER WE NNED TO MOVE Towards your dinner awards :D \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0320d8-505f-4c21-bef8-e4ce63bfa5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ec6f300adb4ddaa415cbfdd9986076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bd22f4-cf91-4bef-9e8f-843c5d771c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"I want to learn about the latest Llama model\"},\n",
    "    ]\n",
    "    \n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9d71b5-13b2-47e5-acb7-0bb058b9948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response: of artificial intelligence model developed by Meta. You're looking for an in-depth analysis of its capabilities, features, and potential applications, is that correct? \n",
      "\n",
      "(Please confirm, and I'll proceed with instructing my team of 5 AI Agent researchers to come up with interesting outlines for the report)\n"
     ]
    }
   ],
   "source": [
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "print(\"Initial response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8473272e-1f2b-44e1-94e5-c8ce1e15df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"\"\"\n",
    "1. I just want to know whats new in Llama 3.3 \n",
    "2. How does it compare with 3.1 \n",
    "3. Executive overview of its comparision with GPT-4o\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e202d6a-0a6c-47f6-904f-3368d1d0e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):]\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032d19b-fe3b-47bb-83ba-0c8feb3b76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d778ca-e881-4680-9bdc-149643c50c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = f\"\"\"\n",
    "Alright Now we enter JSON ONLY RESPONSE mode, here's what you'll do now. Look at your chat history. \n",
    "\n",
    "After getting input from user, we now proceed to starting stating your AI Agent researchers ok?\n",
    "\n",
    "JUST RESPOND in JSON the formatted response, the following details:\n",
    "1. Report Title: This defines the angle of the report your AI Agent will write, your agents aren't as smart as you-you need to give them entire context below\n",
    "2. Context: Give the dumb AI agents the context but its all worth it for your awards\n",
    "3. SYSTEM_Personality: Here we define the personality of each AI Agent researcher, remember to vary them\n",
    "4. Vibe: Fill the vibe to whatever you want\n",
    "5. Energy: Define what energy do you want this AI Agent to achieve\n",
    "6. Goal: Angle of the report\n",
    "\n",
    "Alright, your time to shine-send off {N_OUTLINES}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb6048-9023-4f3c-a02d-be66751f4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, temperature=0.7, top_p=0.9, max_new_tokens=512)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):]\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7e917-f6f9-40d4-ac71-0fb395353a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce947796-1332-4a73-9bbd-4b96ae6bcdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
