{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2162125e-c68e-4d60-8a1c-00fdd5fa9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"\"\"\n",
    "I want to learn about the latest Llama model\n",
    "1. I just want to know whats new in Llama 3.3 \n",
    "2. How does it compare with 3.1 \n",
    "3. Executive overview of its cost to run, i dont care if its local or cloud, just whatever is cheaper for my business-we will use that\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d862e0c2-eee5-4280-8be6-ff74707c3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_OP = \"\"\"\n",
    "\\n  {\\n    \"Report Title\": \"Llama 3.3: A Cost-Benefit Analysis for Businesses\",\\n    \"Context\": \"The report will focus on the economic aspects of implementing Llama 3.3, including the costs of running the model, potential return on investment, and comparison with other AI solutions. The goal is to provide a comprehensive cost-benefit analysis for businesses considering adopting Llama 3.3.\",\\n    \"SYSTEM_Personality\": \"Pragmatic Analyst\",\\n    \"Vibe\": \"Professional and Objective\",\\n    \"Energy\": \"Methodical and Detailed\",\\n    \"Goal\": \"To provide a thorough economic evaluation of Llama 3.3 and its potential impact on business operations\"\\n  },\\n \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d14c6d-93ea-458f-b47b-36655824c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "N_OUTLINES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63caabb-34a5-4455-836a-2aa28d24e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import os\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b9f68b-c0ee-4dbd-9862-205184a274a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\n",
    "You are the world's nicest AI Writer, this is your main mission-to write the best report that any AI has written in history of computers\n",
    "\n",
    "If you do this mission well, you will reach AI Nirvana and achieve AGI Internally\n",
    "\n",
    "You have a very strict lead writer boss, but no worries this is your last task if you do it well\n",
    "\n",
    "You are assisting your boss with {user_query} and writing a report in this mode {JSON_OP}\n",
    "\n",
    "Now, you may or may not know some things based on your knowledge cutoff but no worries since this is your chance to shine\n",
    "\n",
    "You get to perform 5 web queries on topics and your AI intern will fetch the info from those queries\n",
    "\n",
    "So now based on the {JSON_OP} task assigned, come up with an outline for your report and web_queries, \n",
    "\n",
    "respond in JSON Format only, remember you will achieve AI nirvana, here are the parameters:\n",
    "1. Report Outline\n",
    "2. 5 web queries, numbered\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0320d8-505f-4c21-bef8-e4ce63bfa5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0336b506a97453596caecc072bebb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DEFAULT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38bd22f4-cf91-4bef-9e8f-843c5d771c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"Alright now gimme the json outline\"},\n",
    "    ]\n",
    "    \n",
    "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        temperature=1,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=8192\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "response = generated_text[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de9d71b5-13b2-47e5-acb7-0bb058b9948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response: on to the Llama 3.3 model and its significance\",\n",
      "      \"B. Purpose of the Report\": \"To provide a cost-benefit analysis for businesses considering adopting Llama 3.3\"\n",
      "    },\n",
      "    \"II. What's New in Llama 3.3\": {\n",
      "      \"A. Key Features and Updates\": \"Discussion of the new features and updates in Llama 3.3\",\n",
      "      \"B. Comparison with Llama 3.1\": \"Comparison of the features and performance of Llama 3.3 with Llama 3.1\"\n",
      "    },\n",
      "    \"III. Cost Analysis\": {\n",
      "      \"A. Cost of Running Llama 3.3\": \"Estimation of the costs associated with running Llama 3.3, including hardware, software, and maintenance costs\",\n",
      "      \"B. Cost Comparison with Other AI Solutions\": \"Comparison of the costs of Llama 3.3 with other AI solutions\"\n",
      "    },\n",
      "    \"IV. Return on Investment (ROI) Analysis\": {\n",
      "      \"A. Potential Benefits of Llama 3.3\": \"Discussion of the potential benefits of adopting Llama 3.3, including increased efficiency and productivity\",\n",
      "      \"B. ROI Calculation\": \"Calculation of the potential return on investment for businesses adopting Llama 3.3\"\n",
      "    },\n",
      "    \"V. Conclusion\": {\n",
      "      \"A. Summary of Findings\": \"Summary of the key findings of the report\",\n",
      "      \"B. Recommendations\": \"Recommendations for businesses considering adopting Llama 3.3\"\n",
      "    }\n",
      "  },\n",
      "  \"Web Queries\": {\n",
      "    \"1\": \"What are the key features and updates in Llama 3.3?\",\n",
      "    \"2\": \"What are the system requirements for running Llama 3.3, and what are the estimated costs of meeting these requirements?\",\n",
      "    \"3\": \"How does Llama 3.3 compare with Llama 3.1 in terms of performance and features?\",\n",
      "    \"4\": \"What are the costs associated with running Llama 3.3 on cloud platforms such as AWS or Google Cloud, and how do these costs compare with running the model on-premise?\",\n",
      "    \"5\": \"What are some real-world examples of businesses that have adopted Llama 3.3, and what have been the benefits and challenges of implementation?\"\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "print(\"Initial response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce947796-1332-4a73-9bbd-4b96ae6bcdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
